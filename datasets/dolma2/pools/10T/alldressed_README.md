# All-Dressed Pool

This outlines the steps taken to create the webtext, CommonCrawl-sourced, component of the Dolma2 pretraining mix. We dub this pool of data, "All-Dressed". 

## Overview
There are many steps to processing CommonCrawl to be amenable for pretraining, which we will cover in detail. To summarize these steps, we outline the recipe as follows:

* **Linearization:** Commoncrawl provides data in terms of raw web archive data, riddled with HTML tags and other metadata not directly helpful for training. Linearization is the process of text extraction from these WARC/WET files
* **Heuristic Filtering:** To convert the large, linearized dataset into something much more manageable, we apply a round of highly-efficient, but coarse, filtration techniques to remove data that is of low-quality or non-English. 
* **Deduplication:** Duplicate data has been shown to be an inefficient use of training tokens. We apply three rounds of deduplication: first an exact-deduplication to remove identical documents; then a MinHash-based fuzzy deduplication to remove documents that are "nearly identical"; then a suffix-array based deduplication to remove repeated substrings within the surviving documents.
* **Categorization and Quality Partitioning:** We apply two separate classifiers to each remaining document to first classify the data into [WebOrganizer categories](https://arxiv.org/abs/2502.10341) and into quality buckets within each category. This coarse-grained partitioning of the data makes it amenable to mixing and upsampling.
* **Mixing:** The natural data distribution of CommonCrawl isn't optimized for language model pretraining. To rectify this, we filter and upsample documents according to their category and quality bucket from the previous step. 


## Linearization
We start with a 104 CommonCrawl dumps, ranging from 2013 to 2024, with a cutoff-date of Dec 31, 2024. The full list of CommonCrawl dumps used is:
```['CC-MAIN-2013-20', 'CC-MAIN-2013-48', 'CC-MAIN-2014-10', 'CC-MAIN-2014-15', 'CC-MAIN-2014-23', 'CC-MAIN-2014-35', 'CC-MAIN-2014-41', 'CC-MAIN-2014-42', 'CC-MAIN-2014-49', 'CC-MAIN-2014-52', 'CC-MAIN-2015-06', 'CC-MAIN-2015-11', 'CC-MAIN-2015-14', 'CC-MAIN-2015-18', 'CC-MAIN-2015-22', 'CC-MAIN-2015-27', 'CC-MAIN-2015-32', 'CC-MAIN-2015-35', 'CC-MAIN-2015-40', 'CC-MAIN-2015-48', 'CC-MAIN-2016-07', 'CC-MAIN-2016-18', 'CC-MAIN-2016-22', 'CC-MAIN-2016-26', 'CC-MAIN-2016-30', 'CC-MAIN-2016-36', 'CC-MAIN-2016-40', 'CC-MAIN-2016-44', 'CC-MAIN-2016-50', 'CC-MAIN-2017-04', 'CC-MAIN-2017-09', 'CC-MAIN-2017-13', 'CC-MAIN-2017-17', 'CC-MAIN-2017-22', 'CC-MAIN-2017-26', 'CC-MAIN-2017-30', 'CC-MAIN-2017-34', 'CC-MAIN-2017-39', 'CC-MAIN-2017-43', 'CC-MAIN-2017-47', 'CC-MAIN-2017-51', 'CC-MAIN-2018-05', 'CC-MAIN-2018-09', 'CC-MAIN-2018-13', 'CC-MAIN-2018-17', 'CC-MAIN-2018-22', 'CC-MAIN-2018-26', 'CC-MAIN-2018-30', 'CC-MAIN-2018-34', 'CC-MAIN-2018-39', 'CC-MAIN-2018-43', 'CC-MAIN-2018-47', 'CC-MAIN-2018-51', 'CC-MAIN-2019-04', 'CC-MAIN-2019-09', 'CC-MAIN-2019-13', 'CC-MAIN-2019-18', 'CC-MAIN-2019-22', 'CC-MAIN-2019-26', 'CC-MAIN-2019-30', 'CC-MAIN-2019-35', 'CC-MAIN-2019-39', 'CC-MAIN-2019-43', 'CC-MAIN-2019-47', 'CC-MAIN-2019-51', 'CC-MAIN-2020-05', 'CC-MAIN-2020-10', 'CC-MAIN-2020-16', 'CC-MAIN-2020-24', 'CC-MAIN-2020-29', 'CC-MAIN-2020-34', 'CC-MAIN-2020-40', 'CC-MAIN-2020-45', 'CC-MAIN-2020-50', 'CC-MAIN-2021-04', 'CC-MAIN-2021-10', 'CC-MAIN-2021-17', 'CC-MAIN-2021-21', 'CC-MAIN-2021-25', 'CC-MAIN-2021-31', 'CC-MAIN-2021-39', 'CC-MAIN-2021-43', 'CC-MAIN-2021-49', 'CC-MAIN-2022-05', 'CC-MAIN-2022-21', 'CC-MAIN-2022-27', 'CC-MAIN-2022-33', 'CC-MAIN-2022-40', 'CC-MAIN-2022-49', 'CC-MAIN-2023-06', 'CC-MAIN-2023-14', 'CC-MAIN-2023-23', 'CC-MAIN-2023-40', 'CC-MAIN-2023-50', 'CC-MAIN-2024-10', 'CC-MAIN-2024-18', 'CC-MAIN-2024-22', 'CC-MAIN-2024-30', 'CC-MAIN-2024-33', 'CC-MAIN-2024-38', 'CC-MAIN-2024-42', 'CC-MAIN-2024-46', 'CC-MAIN-2024-51', 'CC-MAIN-2024-26']
```
Then we apply [Resiliparse](https://resiliparse.chatnoir.eu/en/stable/) extraction to remove HTML artifacts. For most of these crawls, we use the publicly available Resiliparse-extracted dumps from the [DCLM Pool](https://data.commoncrawl.org/contrib/datacomp/DCLM-pool/index.html). For the remaining dumps, we use the [Dolma-toolkit](https://github.com/allenai/dolma) to extract the text from the WARC files. After this step, we have 255.7B documents in total.

## Heuristic Filtering
Even after HTML artifacts are removed, datasets are often too large and contain mostly low-quality data that is easily identifiable. Following suit of several other common processing pipelines (see RefinedWeb, DCLM, FineWeb), we apply a set of heuristic filters to catch and remove much of this obviously bad data, in addition to performing language filtering to select only. We use a [bespoke native-Rust toolkit](https://github.com/allenai/datamap-rs) to handle these steps. The sub-steps that occur during the heuristic filtering pipeline can be broken down into components like:
* **URL Filtration**: Filter documents based on the contents of their URLs based on various blacklists. This step mostly removes web pages that obviously contain spam or adult content. This removes ~2% of the pool.
* **Basic Heuristics**: This step removes documents that are obviously low-quality. This includes web pages that are too short, too long, don't contain enough alphanumeric characters, or contain too many stopwords. The majority of the pool does not survive this step, with ~64% of the total pool being filtered after this step.
* **Repetition Filter**: We remove documents that contain large amounts of internally repetitious text. This follows the same procedures as in the [Gopher paper](https://arxiv.org/pdf/2112.11446). This step removes ~12% of the total pool. 
* **English Filter**: We then apply an English-language filter to the remaining documents. We use the [`lid.176.bin`](https://fasttext.cc/docs/en/language-identification.html) FastText model to identify the language of each document and keep only documents that have an English score of at least 0.65. This step removes ~3% of the total pool. We note that this is a much smaller removal than the english filtration steps of RefinedWeb or DCLM -- we conjecture this is because we applied cheaper heuristics first: english filtering is comparitively expensive and filtration steps are commutative, so it makes sense to perform the cheaper steps first. 
* **MadLad Filtration**: We apply one last round of filtration, based on the rules for identifying "questionable" sentences from the [Madlad400 paper](https://arxiv.org/pdf/2309.04662). Ablations demonstrated that it was most effective to only apply rules 2 and 5 (identifying sentences with weird up/downcasing and cursed regexes) and remove documents with at least 20% questionable sentences. This removed another ~3% of the total pool. 
* **Modifications**: Finally we applied a round of text-modifiers that do not filter the data but do remove or modify substrings within the corpus, such as many repeated newlines or commonly repeated filler words like "Read more..." or "items in cart". This does not reduce the number of documents in the corpus, but does remove some obvious low-utility text.

After fully applying this pipeline to each of our 104 CommonCrawl shards, we have a corpus of 38.7B documents, an 85% reduction in the input corpus size.


## Deduplication:
We perform deduplication in three rounds of varying coarseness. The first round identifies and removes any documents that are exact copies of each other, down to byte-wise equality over their content strings only. The heuristic filtering step annotates each jsonl with hashes according to the text content, and these hashes are used to perform exact deduplication. We note that exact deduplication can be subsumed entirely by a MinHash fuzzy deduplication step. However, we found that the quantity of exact-duplicates in CommonCrawl is large enough that it is worth it, in terms of compute-efficiency, to perform exact-deduplication to prune the pool before running the more expensive MinHash fuzzy deduplication. 

### Exact Deduplication
During the heuristic filtering phase, we imbued each document with a hash of its text content. We leverage this to remove duplicate content from the dataset by first grouping documents according to this hash-value, such that all documents within a given hash-value live in a known set of files. Then we load each of these groups into memory and keep only one document per unique hash value. This can be easily done with the [Duplodocus tool](https://github.com/allenai/duplodocus), with either the `exact-dedup-memory` or `exact-dedup-disk-...` commands.

This exact deduplication phase removes 67% of the remaining pool, yielding a corpus of 12.8B documents.

### MinHash Deduplication
Next we apply a MinHash deduplication procedure to remove documents that are not exact-duplicates, but are nearly exact-duplicates. Examples of the types of documents that get targeted during a MinHash deduplication phase are documents that contain the same body text but differ in only a few words in the header or footer of the document. We partition the dataset into 32 shards of roughly equal size and perform the following procedure on each step independently. We perform MinHash using an ngram size of 5, and 26 buckets of size 11 for a total of 286 hashes per ngram. This identifies documents with a Jaccard similarity of 0.8 with probability 90%, and a pair of documents with a Jaccard similarity of 0.6 have a 10% chance of being marked as duplicates. 

After identifying and annotating the duplicate cluster of each document, we proceed to check pair of documents in each cluster for their true Jaccard similarity, filtering for Jaccard similarity of 0.8. Amongst clusters of documents that truly have a high Jaccard similarity, we keep only the most recent document, according to crawl-date. This procedure removes a total of 23% of the remaining pool of data. 

This step can be performed by using the `minhash-memory` or `mh-...` commands within the [Duplodocus tool.](https://github.com/allenai/duplodocus)

### Substring Deduplication
Finally we run a substring deduplication procedure to identify and remove repeated substrings that occur within numerous documents in our corpus. This step is intended to remove repetititous boilerplate text, such as repeated headers and footers within web pages. This is performed by sharding the dataset into 56 roughly equal shards and building a suffix array over each shard. Any substring of length at least 500 bytes is then identified and marked for deletion. We apply a novel fuzzy deduplication step where we also remove any short substrings that lie in-between repeated intervals. We also apply the novel step where we ensure we keep at least one copy of each substring within the corpus. Ultimately this step removes 14% of the remaining text.

This step can be performed by using the [bsade tool](https://github.com/liujch1998/bsade/) to identify repeated substrings, and the [datamap-rs tool](https://github.com/allenai/datamap-rs) to perform the fuzzy filtering step.

After all deduplication steps have been applied, we are left with a corpus of 9.7B documents. 

## Categorization and Quality Partitioning
With an aggressively deduplicated corpus of web text, we apply quality and topic classifiers to partition the dataset into buckets of `(topic, quality)` to be used in a fine-grained mixing procedure. To classify documents according to topic, we use the topics defined by [WebOrganizer](https://arxiv.org/abs/2502.10341). Classification is performed using a FastText classifier. To imbue all documents with a quality score, we apply a custom quality classifier, not too dissimilar from that used by DCLM. To further partition each topic bucket into quality buckets, we run a reservoir sample over the quality scores within each bucket and define ranges roughly corresponding to 5-percentile intervals ("vigintiles"). All steps here can be performed by repeated applications of commands within the [datamap-rs tool](https://github.com/allenai/datamap-rs).
